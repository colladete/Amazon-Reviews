{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's upload the data and have a look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('Reviews.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we want to predict positive and negative reviews. I am going to create a new columns with either 'Positive' or 'Negative' as values. Before that, I am going to delete the rows where the score is 3, as this is neither positive nor negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[df_train.Score != 3]\n",
    "df_train['Target'] = 'Pos'\n",
    "df_train['Target'][df_train.Score < 3] = 'Neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be needing most of the columns, so let's get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator',\n",
    "                         'Score', 'Time'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to merge both the 'Summary' and 'Text' column into one column called 'Finaltext'. Then I am going to make sure that Python will not complain later about not being a String. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train['Finaltext'] = df_train.Summary.str.cat(df_train.Text, sep = ' . ')\n",
    "df_train.Finaltext = df_train.Finaltext.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my computer is not the most powerful computer in the world, I am going to work with only 250K of the entries. Feel free to skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_final = df_train[:250000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to separate our data into two datasets: one with all the Positive entries and one with all the negative entries. I will use these two datasets later. Also, note how our classes are unbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pos = df_train_final[df_train_final.Target == 'Pos']\n",
    "df_neg = df_train_final[df_train_final.Target == 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive entries: 210615 Negative entries: 39385\n"
     ]
    }
   ],
   "source": [
    "print('Positive entries:', len(df_pos), 'Negative entries:', len(df_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to code a function that I will use to create the lexicon. The words in this lexicon will be the features in our future dataset. \n",
    "\n",
    "To help cleaning the data we will use:\n",
    "\n",
    "* Stop words: get rid of the most common words that will not help us in our predictions.\n",
    "* Lemmatizer: this function from nltk is helpful to merge similar words.\n",
    "* Exclude the punctuation signs. \n",
    "\n",
    "One importat thing to notice:\n",
    "\n",
    "I am excluding the most popular words and the least popular words. As you can see in the code below, I am keeping the words that appear at least once every 150 entries. You can play with this number, the smaller it is, the fewer features you will get and probably less accuracy later, but it also means a decrease in the running time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def create_lexicon(data):\n",
    "    lexicon = []\n",
    "    for lines in data:\n",
    "        if type(lines) is str:\n",
    "            words = word_tokenize(lines.lower())\n",
    "            lexicon += [w for w in words if w not in [stop_words,exclude]]\n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "    word_count = Counter(lexicon)\n",
    "    final_lexicon = []\n",
    "    for word in word_count:\n",
    "        if (len(data) / 2) > word_count[word] > (len(data)/150):\n",
    "            final_lexicon.append(word)\n",
    "    return final_lexicon, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our lexicon. This step might take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_amazon, word_count_amazon = create_lexicon(df_train_final.Finaltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 1157 words in our lexicon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'quality', 'dog', 'food', 'bought', 'several', 'canned', 'product', 'found', 'them', 'all', 'be', 'look', 'more', 'like', 'than', 'meat', 'smell', 'better', 'she']\n"
     ]
    }
   ],
   "source": [
    "print(lexicon_amazon[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will take the positive entries, the negative entries, the lexicon and then create a combined dataset with all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(pos,neg,lexicon):\n",
    "    dataset = []\n",
    "    for lp in pos:\n",
    "        words_pos = word_tokenize(lp)\n",
    "        if type(words_pos) is str:\n",
    "            words_pos = words_pos.lower()\n",
    "        words_pos = [lemmatizer.lemmatize(i) for i in words_pos]\n",
    "        features_pos = np.zeros(len(lexicon) + 2)\n",
    "        features_pos[-1] = 1\n",
    "        for word in words_pos:\n",
    "            if word in lexicon:\n",
    "                index = lexicon.index(word)\n",
    "                features_pos[index] += 1\n",
    "        dataset.append(features_pos)\n",
    "    \n",
    "    for ln in neg:\n",
    "        words_neg = word_tokenize(ln)\n",
    "        if type(words_neg) is str:\n",
    "            words_neg = words_neg.lower()\n",
    "        words_neg = [lemmatizer.lemmatize(i) for i in words_neg]\n",
    "        features_neg = np.zeros(len(lexicon) + 2)\n",
    "        features_neg[-1] = 0\n",
    "        for word in words_neg:\n",
    "            if word in lexicon:\n",
    "                index = lexicon.index(word)\n",
    "                features_neg[index] += 1\n",
    "        dataset.append(features_neg)\n",
    "    dataset = np.array(dataset)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our final dataset! This step will definitely take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = create_dataset(df_pos.Finaltext, df_neg.Finaltext, lexicon_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we divide the data into X, y and T. y is the target and T is the target as one hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "def y2indicator(y):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, 2))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "T = y2indicator(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's divide the X, y and T into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, T_train, T_test = train_test_split(X, y, T, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before coding the Neural Network, we can play with several 'simpler' algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90834000000000004"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvc = LinearSVC()\n",
    "lvc.fit(X_train, y_train)\n",
    "lvc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88544"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbmod = MultinomialNB()\n",
    "mnbmod.fit(X_train, y_train)\n",
    "mnbmod.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82179999999999997"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnbmod = GaussianNB()\n",
    "gnbmod.fit(X_train, y_train)\n",
    "gnbmod.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85633999999999999"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnbmod = BernoulliNB()\n",
    "bnbmod.fit(X_train, y_train)\n",
    "bnbmod.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88983333333333337"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "logistic.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the Neural Network. I explain it inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost / err at iteration i=0, j=0: 34529.641 / 0.810\n",
      "Cost / err at iteration i=0, j=50: 18442.072 / 0.842\n",
      "Cost / err at iteration i=0, j=100: 13726.021 / 0.872\n",
      "Cost / err at iteration i=1, j=0: 12387.218 / 0.902\n",
      "Cost / err at iteration i=1, j=50: 11408.704 / 0.909\n",
      "Cost / err at iteration i=1, j=100: 10991.662 / 0.912\n",
      "Cost / err at iteration i=2, j=0: 10900.015 / 0.912\n",
      "Cost / err at iteration i=2, j=50: 10676.706 / 0.914\n",
      "Cost / err at iteration i=2, j=100: 10434.529 / 0.916\n",
      "Cost / err at iteration i=3, j=0: 10364.512 / 0.916\n",
      "Cost / err at iteration i=3, j=50: 10103.355 / 0.919\n",
      "Cost / err at iteration i=3, j=100: 9790.184 / 0.922\n",
      "Cost / err at iteration i=4, j=0: 9652.279 / 0.924\n",
      "Cost / err at iteration i=4, j=50: 9553.721 / 0.925\n",
      "Cost / err at iteration i=4, j=100: 9130.189 / 0.929\n",
      "Cost / err at iteration i=5, j=0: 9214.127 / 0.931\n",
      "Cost / err at iteration i=5, j=50: 9502.551 / 0.929\n",
      "Cost / err at iteration i=5, j=100: 9595.035 / 0.934\n"
     ]
    }
   ],
   "source": [
    "#First, we create this function to calculate the accuracy. \n",
    "\n",
    "def accuracy(p, t):\n",
    "    accuracy = np.mean(p == t)\n",
    "    return accuracy\n",
    "\n",
    "#Now let's define some parameters of our NN. You can defintely play with these to improve the speed and accuracy. \n",
    "\n",
    "max_iter = 6\n",
    "print_period = 50\n",
    "lr = 0.00002\n",
    "reg = 0.001\n",
    "\n",
    "N, D = X_train.shape\n",
    "batch_sz = 1500\n",
    "n_batches = int(N / batch_sz)\n",
    "\n",
    "#I will be using one NN with 3 layers of 500 hidden nodes each. \n",
    "\n",
    "M1 = 500\n",
    "M2 = 500\n",
    "M3 = 500\n",
    "K = 2\n",
    "\n",
    "#These are the values to initialize the weights and biases. \n",
    "\n",
    "W1_init = np.random.randn(D, M1) / np.sqrt(N)\n",
    "b1_init = np.zeros(M1)\n",
    "W2_init = np.random.randn(M1, M2) / np.sqrt(M1)\n",
    "b2_init = np.zeros(M2)\n",
    "W3_init = np.random.randn(M2, M3) / np.sqrt(M2)\n",
    "b3_init = np.zeros(M3)\n",
    "W4_init = np.random.randn(M3, K) / np.sqrt(M3)\n",
    "b4_init = np.zeros(K)\n",
    "\n",
    "#And now these are the tf variables. \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "T = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "W1 = tf.Variable(W1_init.astype(np.float32))\n",
    "b1 = tf.Variable(b1_init.astype(np.float32))\n",
    "W2 = tf.Variable(W2_init.astype(np.float32))\n",
    "b2 = tf.Variable(b2_init.astype(np.float32))\n",
    "W3 = tf.Variable(W3_init.astype(np.float32))\n",
    "b3 = tf.Variable(b3_init.astype(np.float32))\n",
    "W4 = tf.Variable(W4_init.astype(np.float32))\n",
    "b4 = tf.Variable(b4_init.astype(np.float32))\n",
    "\n",
    "#These are the activation values. I am using relu but this can be changed as well. \n",
    "\n",
    "Z1 = tf.nn.relu( tf.matmul(X, W1) + b1 )\n",
    "Z2 = tf.nn.relu( tf.matmul(Z1, W2) + b2 )\n",
    "Z3 = tf.nn.relu( tf.matmul(Z2, W3) + b3 )\n",
    "Yish = tf.matmul(Z3, W4) + b4 \n",
    "\n",
    "\n",
    "#This is our cost function, with will use Softmax. \n",
    "\n",
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=Yish, labels=T))\n",
    "\n",
    "#This line is for the optimizer. I am using RMSProp as it allows for momentum, but feel free to change it as well. \n",
    "\n",
    "train_op = tf.train.RMSPropOptimizer(lr, decay=0.99, momentum=0.9).minimize(cost)\n",
    "\n",
    "#This is our prediction line\n",
    "\n",
    "predict_op = tf.argmax(Yish, 1)\n",
    "\n",
    "#And now we can start!\n",
    "\n",
    "costs = []\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    for i in range(0,max_iter):\n",
    "        for j in range(0,n_batches):\n",
    "            Xbatch = X_train[j*batch_sz:(j*batch_sz + batch_sz),:]\n",
    "            Ybatch = T_train[j*batch_sz:(j*batch_sz + batch_sz),:]\n",
    "\n",
    "            session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})\n",
    "            if j % print_period == 0:\n",
    "                test_cost = session.run(cost, feed_dict={X: X_test, T: T_test})\n",
    "                prediction = session.run(predict_op, feed_dict={X: X_test})\n",
    "                acc = accuracy(prediction, y_test)\n",
    "                print (\"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, test_cost, acc))\n",
    "                costs.append(test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.83      0.78      6977\n",
      "          1       0.97      0.95      0.96     43023\n",
      "\n",
      "avg / total       0.94      0.93      0.94     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(prediction, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
